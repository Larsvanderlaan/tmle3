---
title: "tmle3 Design"
author: "Jeremy Coyle"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{tmle3 Design}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r setup, echo=FALSE}
library(knitr)

# Define insert_fun.
insert_fun = function(name) {
  read_chunk(lines = capture.output(dump(name, '')), labels = paste(name, 'source', sep = '-'))
}
```

# Introduction

tmle is defined by:
* a parameter of interest
* a likelihood fit
* a submodel
* a loss function

spec components:
* `tmle_task <- tmle_spec$make_tmle_task(data, node_list)`
*  `likelihood <- tmle_spec$make_likelihood(tmle_task, learner_list)`
*  `tmle_params <- tmle_spec$make_params(tmle_task, likelihood)`
*  `updater <- tmle_spec$make_updater(likelihood, tmle_params)`
* `fit <- fit_tmle3(tmle_task, likelihood, tmle_params, updater)`
  
roadmap:
* data (raw data + npsem)
* model (kind of implicit in parameter for now)
* target parameter
* super learner
* tmle
* inference
* interpretation

The `tmle3` package attempts to directly model the key objects required to define and implement a TMLE estimator, while being general enough to support the wide variety of applications of the general TMLE concept present in the literature. These objects include the NPSEM, factorized likelihood, counterfactual interventions, parameters and their EIFs and clever covariates, parametric submodels, and loss function. To introduce how these concepts are defined in the `tmle` package, we work through a simple example below.

We use data from the Collaborative Perinatal Project (CPP), available in the `sl3` package. To simplify this example, we define a binary intervention variable, `parity01` -- an indicator of having one or more children before the current child and a binary outcome, `haz01` -- an indicator of having an above average height for age. We do this because the current implementation of `tmle3` is limited to binary exposures and outcomes, although this is not a design limitation.

```{r prelims}
library(tmle3)
library(sl3)
data(cpp)
cpp <- cpp[!is.na(cpp[, "haz"]), ]
cpp$parity01 <- as.numeric(cpp$parity > 0)
cpp[is.na(cpp)] <- 0
cpp$haz01 <- as.numeric(cpp$haz > 0)
```
<!-- # TMLE Workflow -->

<!-- We now work through the steps to define and estimate the TMLE for a treatment specific mean (TSM) parameter on these data. Currently this requires the user to manually define a lot of objects, which illustrates the architecture of the package. We envision that these definitions could be bundled into a full defined "tmle" object that can then be defined to arbitrary datasets with minimal input from the user. -->

<!-- ### Define NPSEM (`tmle3_Node` object) -->

<!-- The first step is to define a `nodes` object that defines the roles and relationships between the variables: -->

<!-- ``` {r define_npsem} -->
<!-- tmle_nodes <- list( -->
<!--   define_node("W", c( -->
<!--     "apgar1", "apgar5", "gagebrth", "mage", -->
<!--     "meducyrs", "sexn" -->
<!--   )), -->
<!--   define_node("A", c("parity01"), c("W")), -->
<!--   define_node("Y", c("haz01"), c("A", "W")) -->
<!-- ) -->
<!-- ``` -->

<!-- Each node is defined by a node name (here `W`, `A`, and `Y`, a list of variables that comprise the node, and a list of parent nodes). Nodes also track information about the data types of the variables (continuous, categorical, binomial, etc). Here, that information is being estimated manually from the data. Going forward, each node will also contain information about censoring indicators, where applicable. Censoring indicators have not yet been implemented. -->

<!-- ### Define Data (`tmle3_Task` object) -->

<!-- Next, we define a `tmle3_Task` object that contains both the data and the NPSEM. This is an extension of the `sl3_Task` object that we used to define a regression task: -->

<!-- ```{r define_task} -->
<!-- task <- tmle3_Task$new(cpp, tmle_nodes = tmle_nodes) -->
<!-- ``` -->

<!-- This task object contains methods to help subset the data as needed for various tmle steps: -->

<!-- ```{r subset_task} -->
<!-- #get the outcome task -->
<!-- head(task$get_tmle_node("Y")) -->

<!-- #get the sl3 task corresponding to an outcome regression -->
<!-- task$get_regression_task("Y") -->
<!-- ``` -->

<!-- ### Define Likelihood (`Likelihood` object) -->

<!-- Next, we define a `Likelihood` object corresponding to a the relevant factors of the factorized likelihood, and specify `sl3` learners as estimators for the likelihood factors: -->

<!-- ```{r likelihood} -->

<!-- # set up sl3 learners for tmle3 fit -->
<!-- lrnr_glm_fast <- make_learner(Lrnr_glm_fast) -->
<!-- lrnr_mean <- make_learner(Lrnr_mean) -->

<!-- # define and fit likelihood -->
<!-- factor_list <- list( -->
<!--   define_lf(LF_static, "W", NA), -->
<!--   define_lf(LF_fit, "A", lrnr_glm_fast), -->
<!--   define_lf(LF_fit, "Y", lrnr_mean) -->
<!-- ) -->
<!-- ``` -->

<!-- The `Likelihood` class is actually an extension of the `sl3` `Lrnr_base` base class, and so has methods for fitting and generating predictions from a likelihood specification and a dataset. Above, we see two types of likelihood factors `LF_fit`, specifies a factor that will be estimated from data using `sl3`, and `LF_static`, specifies a degenerate likelihood factor that takes a single value with probability one. Here, we use this for `W`, even though we are of course estimating the marginal density of `W` using the empirical likelihood. This is an implementation quirk that needs to be fixed. Additional likelihood factors types will be defined for things like known stochastic and rule based variables. -->

<!-- We then fit those likelihood estimators to the `tmle3_Task` defined above, obtaining a fit likelihood estimate: -->

<!-- ```{r likelihood fit} -->
<!-- likelihood_def <- Likelihood$new(factor_list) -->
<!-- likelihood <- likelihood_def$train(task) -->
<!-- print(likelihood) -->
<!-- ``` -->

<!-- ### Define Counterfactual (`Counterfactual` object) -->

<!-- Next, we define a counterfactual object comprised of alternative likelihood factors to replace those specified above: -->
<!-- ```{r counterfactual} -->
<!-- intervention <- define_cf(define_lf(LF_static, "A", value = 1)) -->
<!-- ``` -->

<!-- This allows us to model a counterfactual likelihood as a likelihood where certain factors are replaced with their intervention counterparts: -->

<!-- ```{r counterfactual_likelihood} -->
<!-- cf_likelihood <- intervention$cf_likelihood(likelihood) -->
<!-- print(cf_likelihood) -->
<!-- ``` -->


<!-- ### Define Parameter (`Param_base` object and its children) -->

<!-- Next we define the parameter of interest, here a `Param_TSM` treatment specific mean parameter, which we can now define for arbitrary interventions. -->

<!-- ```{r parameter} -->
<!-- tsm <- Param_TSM$new(intervention) -->
<!-- ``` -->

<!-- ```{r cache=FALSE, echo=FALSE} -->
<!-- chunkfile <- "../R/Param_TSM.R" -->
<!-- if (!file.exists(chunkfile)) { -->
<!--   #because pkgdown -->
<!--   chunkfile <- "../../R/Param_TSM.R" -->
<!-- } -->
<!-- read_chunk(chunkfile, label="param_TSM") -->
<!-- ``` -->

<!-- Let's take a look at how parameters are defined: -->

<!-- ```{r param_TSM, eval=FALSE} -->
<!-- ``` -->

<!-- There's two key methods here. First, `Param_TSM$HA`, which defines the clever covariate to fluctuate the factor of the likelihood corresponding to the conditional mean of $Y$ on $A$ and $W$. Currently, this limits `tmle3` to TMLEs that only fluctuate that factor. Of course, this needs to be expanded to support fluctuating any factor. This can be done by simply modifying the parameter definition to support multiple clever covariates. -->

<!-- Second, `Param_TSM$estimates` defines the mapping from the data to estimates of the parameter and EIF. -->

<!-- ### Define submodel and loss function (modeled using a `sl3` Learner) -->

<!-- The submodel and loss function together define an optimization problem that we define using a `sl3` learner. Here are two example definitions: -->

<!-- ``` {r submodels} -->
<!-- # optimize using glm (logistic submodel, logistic log likelihood loss) -->
<!-- lrnr_submodel <- make_learner(Lrnr_glm_fast, intercept = FALSE, transform_offset = TRUE) -->

<!-- # same submodel and loss function, but directly using numerical optimization -->
<!-- print(submodel_logit) -->
<!-- print(sl3:::loss_loglik_binomial) -->
<!-- lrnr_submodel2 <- make_learner(Lrnr_optim, submodel_logit, sl3:::loss_loglik_binomial, -->
<!--                              init_0 = TRUE) -->
<!-- ``` -->

<!-- ### Fit submodel and update likelihood -->

<!-- Finally, we can use all the components we defined above to fit the submodel to the relevant data and obtain a updated likelihood estimate, targeted to the parameter of interest: -->

<!-- ```{r tmle_likelihood} -->
<!-- tmle_likelihood <- fit_tmle_likelihood(likelihood, task, tsm, lrnr_submodel) -->
<!-- ``` -->

<!-- ### Obtain parameter estimates -->

<!-- We can now get estimates of our parameter from both the initial and targeted likelihood estimates: -->

<!-- ``` {r estimates} -->
<!-- init_ests <- tsm$estimates(likelihood, task) -->
<!-- tmle_ests <- tsm$estimates(tmle_likelihood, task) -->

<!-- # get initial and tmle estimates -->
<!-- print(init_ests$psi) -->
<!-- print(tmle_ests$psi) -->
<!-- print(mean(init_ests$IC)) -->
<!-- print(mean(tmle_ests$IC)) -->
<!-- ``` -->

### Bundling a TMLE definition for the end user

`tmle_spec`

* How to document these

### The Delta method

# Future Work

* Support for fluctuating other likelihood factors
* TMLE for multiple parameters
* One-step (recursive TMLE)
* Weights-based fluctuation
* Support for dynamic rule and stochastic interventions
* Extension to longitudinal case
* Simplified user interface