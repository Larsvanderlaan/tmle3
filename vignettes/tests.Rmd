---
title: "tests"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r, include = F}

devtools::document()
library(sl3)
```

```{r}
library(simcausal)
D <- DAG.empty()
D <- D +
  node("L0", distr = "rbinom", size =1, prob = 0.5) +
  node("A0", distr = "rbinom", size = 1, prob = 0.5 + L0/4 ) +
  node("L1",  distr =  "rbinom", size = 1, prob = 0.5 + A0/4 ) +
  node("A1", distr = "rbinom", size = 1, prob = 0.5 + L1/4 ) +
  node("L2",  distr =  "rbinom", size = 1, prob = 0.5 + A1/4 ) +
  node("A2", distr = "rbinom", size = 1, prob = 0.5 + L2/4 ) +
  node("Y", distr = "rbinom", size = 1, prob = 0.5 + (A2 + L2)/4 ) 
 
setD <- set.DAG(D)
dat <- sim(setD, n = 1e3)
dat$id <- dat$ID
dat$ID <- NULL
```

```{r}

long_data <- rbindlist(lapply(0:2, function(i){
  dat <- copy(dat[, c(grep(paste(i), colnames(dat), value = T), "id", "Y")])
  setnames(dat, c("L", "A", "id", "Y"))
  dat$t <- i
  return(dat)
}))
setkey(long_data, id , t)
long_data

# Longitudinal data with growing past
npsem <- list(define_node("L0", c("L"),c(), time = 0), define_node("A0", c("A"), c("L0"), time = 0) ,
     define_node("L1", c("L"),c("A0", "L1"), time = 1), define_node("A1", c("A"), c("L0", "L1", "A0"), time = 1) ,
     define_node("L2", c("L"),c("L1", "L0", "A1", "A0"), time = 2), define_node("A2", c("A"), c("L0", "L1", "L2", "A0", "A1"), time = 2), define_node("Y", c("Y"), c("L0", "L1", "L2", "A0", "A1", "A2"), time = 2))

task <- tmle3_Task$new(long_data, npsem, time = "t", id = "id")

task$get_regression_task("L2")$data
```

```{r}
npsem <- list(define_node("L0", c("L"),c(), time = 0), define_node("A0", c("A"), c("L0"), time = 0) ,
     define_node("L1", c("L"),c("A0", "L0"), time = 1), define_node("A1", c("A"), c("L1", "A0"), time = 1) ,
     define_node("L2", c("L"),c("L1", "A1"), time = 2), define_node("A2", c("A"), c( "L2","A1"), time = 2), define_node("Y", c("Y"), c("L0", "L1", "L2", "A0", "A1", "A2"), time = 2))

task <- tmle3_Task$new(long_data, npsem, time = "t", id = "id")


#Check names are equal up to order for pooling
assertthat::assert_that(identical(sort(names(task$get_regression_task("L2")$data)), sort(names(task$get_regression_task("L1")$data))))
assertthat::assert_that(identical(sort(names(task$get_regression_task("A2")$data)), sort(names(task$get_regression_task("A1")$data))))

factor_list <- list(LF_emp$new("L0"), 
                    LF_fit$new("A0", make_learner(Lrnr_glm_fast)),
                    LF_fit$new(c("L1", "L2"), make_learner(Lrnr_glm_fast)),
                    LF_fit$new(c("A1", "A2"), make_learner(Lrnr_glm_fast)),
                    LF_fit$new("Y", make_learner(Lrnr_glm_fast))
                    )
```

```{r}
lik <- Likelihood$new(factor_list)
assertthat::assert_that(length(lik$factor_list_pooled) == length(factor_list))
assertthat::assert_that(length(lik$factor_list) == length(task$npsem))
lik_trained <- lik$train(task)
```

```{r}
task <- tmle3_Task$new(long_data, npsem, time = "t", id = "id")

# Check pooled regression tasks coincide (up to order of rows due to id and time)
pooled = task$get_regression_task(c("L1", "L2"), expand = T)$data
stacked = rbindlist(list(task$get_regression_task(c("L1"), expand = T, is_time_variant =  T)$data,
task$get_regression_task(c("L2"), expand = T, is_time_variant = T)$data), use.names = F)
setkey(stacked, id , t)
setkey(pooled, id , t)
assertthat::assert_that(all(stacked == pooled, use.names = T))


l1 <- lik$get_likelihood(task, "L1", drop_id = F, drop_time = T, to_wide = F)
l2 <- lik$get_likelihood(task, "L2", drop_id = F, drop_time = T, to_wide = F)
l12 <- lik$get_likelihood(task, c("L1", "L2"), drop_id = F, drop_time = F, to_wide = T)
merged_l12 = merge(l1, l2, by = c("id"))
# Check that pooled and unpooled predictions coincide
assertthat::assert_that(all(l12[, c("L1", "L2")] ==  merged_l12[, c("L1", "L2")]))
```

```{r}
lik$get_likelihood(task, "L1", drop_id = F, drop_time = F)
```

```{r}
task$data
generator <-function(task, Likelihood = NULL, target_param = NULL, node, outcome = T){
  task <- task$get_regression_task(node)
  cols <- task$add_columns(data.table(IC = rnorm(task$nrow)))
  task$next_in_chain(column_names = cols, covariates  = c(task$nodes$covariates, task$nodes$outcome), outcome = "IC")
}
generator(task, node = "Y")$data
```

```{r}
library(data.table)

# Generate simple causal model
library(simcausal)

D <- DAG.empty()
D <- D +
  node("W", distr = "runif", min =-1, max = 1) +
  node("A", distr = "rbinom", size = 1, prob = 0.5 + W/4 ) +
  node("T", distr = "rexp", rate = 0.05*(1 + 0.5*A - 0.3*W) )+
  node("C", distr = "rexp",  rate = 0.03*(1 + 0.4*A - 0.2*W)) +
  node("Delta", distr = "rconst", const = as.numeric(T<=C)  ) +
  node("Ttilde", distr = "rconst", const = round(min(T,C,10) +1  ))
setD <- set.DAG(D)
dat <- sim(setD, n = 1e2)
# only grab ID, W's, A, T.tilde, Delta
Lname <- grep("W", colnames(dat), value = TRUE)
Aname <- grep("A", colnames(dat), value = TRUE)
df <- dat[, c("ID", Lname, Aname, "T", "C", "Delta", "Ttilde")]

data <- df[,c( "W", "A", "Ttilde", "Delta")]
data$id = df$ID

data$t <- data$Ttilde 
data = data.table(data)
data$processA = as.numeric(data$Delta ==0)
data$processN = as.numeric(data$Delta ==1)
data_baseline <- data
data_baseline$t =0
data_baseline$processA = 0
data_baseline$processN = 0
data_baseline$Ttilde = NA
data_baseline$Delta = NA
data = rbind(data_baseline, data)
data$Ttilde = NULL
data$Delta = NULL
data$at_risk = 1 - (data$processA + data$processN)
data[which(data$processN==1), "processA"] = 0
npsem = list(define_node("W", c("W"), time =0), 
     define_node("A", c("A"), c("W"), time =0, summary_functions = make_summary_measure_baseline(c("W"))),
     
     define_node("processA", c("processA"),  c("A","W"), time = seq(1,max(data$t)), summary_functions = make_summary_measure_baseline(c("A", "W")),
                  risk_set_map = make_summary_measure_last_value("at_risk", strict_past  = T), missing_row_implies_not_at_risk = F),
     
define_node("processN", c("processN"), c("A","W"),time = seq(1,max(data$t)), summary_functions = make_summary_measure_baseline(c("A", "W")),
                  risk_set_map = make_summary_measure_last_value("at_risk", strict_past = T), missing_row_implies_not_at_risk = F))
# Delta is the counting process which is 1 at death
data
```


 
```{r}
c(T,T, F) && T
```

```{r}
library(sl3)
devtools::document()

```


```{r}
n=500
way = data.table(W = (rbinom(n, size = 50, 0.5)), A = (rbinom(n, size = 50, 0.5)), Y = (rbinom(n, size = 300, 0.5)))
way
generator <-function(task, Likelihood = NULL, target_param = NULL, node, outcome = T){
  task <- task$get_regression_task(node)
  cols <- task$add_columns(data.table(Y= as.numeric(as.character(task$Y)) , IC = as.numeric(as.character(task$Y)) + rowSums(task$X)))
  task$next_in_chain( column_names = cols, covariates  = c(task$nodes$covariates, task$nodes$outcome), outcome = "IC")
}

library(data.table)
npsem = list(define_node("W", c("W")), define_node("Y", c("Y"), c("A", "W"), variable_type = variable_type(type = "categorical", levels = sort(unique(way$Y)))),  define_node("A", c("A"), "W"))


task <- tmle3_Task$new(as.data.table(way), npsem)
way
task$get_regression_task("Y")$data
task$npsem$Y$variable_type$type
library(sl3)
likelihood_factors <- list("W" = LF_emp$new("W" ), "A" = LF_fit$new("A", make_learner(Lrnr_condensier)), "Y" =  LF_fit$new("Y", make_learner(Lrnr_pooled_hazards), type = "density"))
liks <- Likelihood$new(likelihood_factors)


liks = liks$train(task)
grad <- Gradient$new(liks, generator, list(update_nodes = "Y"))

grad <- grad$train(task)
grad$compute_component(task, "Y")

```

```{r}
setattr(task, "target_nodes", "Y")
    AA~e1d  fqfv`deGBBG

```
```{r}
levels = sort(unique(way$trueY))

way$trueid <- seq_len(500)
way$trueY <- way$Y
w=rbindlist(lapply(levels, function(level) {
  way <- copy(way)
  set(way ,, "Y", level)
  way
}))
w$id <- paste(w$trueid, w$Y, sep = "_")
w
expand_task = function(tmle_task, node){
  variables <- tmle_task$npsem[[node]]$variables
  if(length(variables) >1) stop("Multivariate nodes not supported")
  data <- tmle_task$data
  data$trueid <- data$id
  levels <- sort(unique(unlist(data[, variables, with = F])))
  long_data <- rbindlist(lapply(levels, function(level) {
    data <- copy(data)
    set(data ,, variables, level)
    return(data)
  }))
  long_data$id <-  paste(long_data$trueid,long_data[, variables, with = F][[1]], sep = "_")
  long_task <- tmle3_Task$new(long_data, tmle_task$npsem, id = "id", time = "t", force_at_risk = tmle_task$force_at_risk, summary_measure_columns = c(tmle_task$summary_measure_columns, "trueid"))
  return(long_task)
}
expand_task(task, "Y")$data
```

```{r}
task$force_at_risk
```
```{r}
grad <- Gradient$new(liks, generator, list(update_nodes = "Y"))

grad <- grad$train(task)
out = (grad$compute_component(task, "Y"))
lapply(out, as.data.table)
```



```{r}

preds <- liks$factor_list$Y$get_density(task, "full", quick_pred = T)
levels <- task$npsem$Y$variable_type$levels
preds <- data.table(sl3::unpack_predictions(as.vector(preds)))
preds
setnames(preds, as.character(seq_along(levels)))
preds
cdf <- data.table(t(apply(preds, 1, cumsum)))
X = as.matrix(cbind(task$get_regression_task("Y")$X, as.numeric(as.character(task$get_regression_task("Y")$Y))))
basis_list = hal9001::enumerate_basis(X, 1)
data.table(X)
design1 <- as.data.table(as.matrix(hal9001::make_design_matrix(X, basis_list)))
```




```{r}
design <- copy(design1)
# Design matrix where Y part of indicator always evaluated to 1
clean_basis <- function(basis){
  if(!(3 %in% basis$cols)){
    return(basis)
  }
  index = which(basis$cols == 3)
  basis$cutoffs[index] <- -100
  return(basis)
}
clean_list = lapply(basis_list, clean_basis)
clean_design <- hal9001::make_design_matrix(X, clean_list)
clean_design <- data.table(as.matrix(clean_design))



# Get basis functions to change and their cdf column
diff_map <- unlist(lapply(seq_along(basis_list), function(i) {
  basis <- basis_list[[i]]
  if(!(3 %in% basis$cols)){
    return(NULL)
  }
  result <- (list(which(levels == basis$cutoffs[which(basis$cols == 3)])))
  names(result) = i
  return(result)
}))

donothing <- lapply((names(diff_map)), function(i){
  col_index <- diff_map[[i]]
  diff <- design[[as.integer(i)]] - 1 + cdf[[col_index]] 
  set(design, , as.integer(i), diff)
})

result = (design* clean_design)
dim(result)
```



```{r}
grad <- Gradient$new(liks, generator, list(update_nodes = "Y"))

grad <- grad$train(task)
grad$compute_component(task, "Y")
```

```{r}
grad$compute_component(task, "Y")

```

```{r}
n = 5
task[n]$data
i = 23
grad$fit_object$Y$basis_list[[i]]
b = grad$basis[["Y"]][[i]]
b(task[n], "full")
```







